{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Pedro Alonso Lopez Torres**\n",
    "- **Elisa Ottoboni**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Expectation values for ordinary least squares expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Request 1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show that the expectation value of $y$ for a given element $i$\n",
    "\n",
    "$$\n",
    "\\mathbb{E}(y_i) = \\sum_j x_{ij} \\beta_j = \\mathbf{X}_{i,*} \\boldsymbol{\\beta}\n",
    "$$\n",
    "\n",
    "and that its variance is\n",
    "\n",
    "$$\n",
    "\\text{Var}(y_i) = \\sigma^2\n",
    "$$\n",
    "\n",
    "Hence, $y_i \\sim \\mathcal{N}(\\mathbf{X}_{i,*} \\boldsymbol{\\beta}, \\sigma^2)$, that is $y$ follows a normal distribution with mean value $\\mathbf{X} \\boldsymbol{\\beta}$ and variance $\\sigma^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by writing $y_i$ in its matrix form:\n",
    "\n",
    "$$\n",
    "y_i = \\mathbf{X}_{i,*} \\boldsymbol{\\beta} + \\epsilon_i\n",
    "$$\n",
    "where $\\mathbf{X}_{i,*}$ represents the $i$-th row of matrix $\\mathbf{X}$, $\\boldsymbol{\\beta}$ the parameters vector, and $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$.\n",
    "\n",
    "Thanks to the linearity of expectation, we can write the expectation of $y_i$ as follows:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}(y_i) = \\mathbb{E}({\\mathbf{X}_{i,*}}) + \\mathbb{E}(\\epsilon_i)\n",
    "$$\n",
    "Given that $\\mathbf{X}_{i,*}$ represents a non-stochastic term,  its expectation is simply the term itself. On the other hand, $\\mathbb{E}(\\epsilon_i)$ is equal to $0$, so we obtain:\n",
    "$$\n",
    "\\mathbb{E}(y_i) = \\mathbf{X}_{i,*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterwards, we have to show that $\\text{Var}(y_i) = \\sigma^2$. Again, we start by writing $y_i$ equation:\n",
    "\n",
    "$$\n",
    "y_i = \\mathbf{X}_{i,*} \\boldsymbol{\\beta} + \\epsilon_i\n",
    "$$ยง\n",
    "\n",
    "The term $\\mathbf{X}_{i,*}$ is deterministic, so its variance is 0. Hence, the variance of $y_i$ comes solely from the random variable $\\epsilon_i$, which has variance equal to $\\sigma^2$, obtaining:\n",
    "\n",
    "$$\n",
    "\\text{Var}(y_i) = \\text{Var}(\\epsilon_i) = \\sigma^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having calculated the expectation and the variance of $y_i$, we can express it as a normal distribution\n",
    "\n",
    "$$\n",
    "y_i \\sim \\mathcal{N}(\\mathbf{X}_{i,*} \\boldsymbol{\\beta}, \\sigma^2)\n",
    "$$\n",
    "Given that we added a constant $\\mathbf{X}_{i,*}$ to a normal distribution represented by $\\epsilon_i$, its variance remains the same, but its mean changes according to the constant value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Request 1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the OLS expressions for the optimal parameters $\\hat{\\boldsymbol{\\beta}}$ show that\n",
    "\n",
    "$$\n",
    "\\mathbb{E}(\\hat{\\boldsymbol{\\beta}}) = \\boldsymbol{\\beta}\n",
    "$$\n",
    "\n",
    "Show finally that the variance of $\\boldsymbol{\\beta}$ is\n",
    "\n",
    "$$\n",
    "\\text{Var}(\\hat{\\boldsymbol{\\beta}}) = \\sigma^2 (\\mathbf{X}^T \\mathbf{X})^{-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by writing the expression for  the ordinary least squares (OLS) estimator for $\\boldsymbol{\\hat{\\beta}}$:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\hat{\\beta}} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{X}$ is the design matrix and $\\mathbf{y}$ is the vector of observed values.\n",
    "\n",
    "We know that the linear model we are using is expressed by:\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\n",
    "$$\n",
    "\n",
    "where $\\boldsymbol{\\beta}$ is the parameters vector, and $\\boldsymbol{\\epsilon}$ is the normally distributed noise.\n",
    "\n",
    "We substitute $\\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}$ into the OLS estimator for $\\boldsymbol{\\hat{\\beta}}$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\boldsymbol{\\hat{\\beta}}\n",
    "& = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T (\\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}) \\\\\n",
    "& = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{X} \\boldsymbol{\\beta} + (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\epsilon} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "The first term of the addition simplifies to the identity matrix $\\mathbf{I}$:\n",
    "\n",
    "$$\n",
    "(\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{X} = \\mathbf{I}\n",
    "$$\n",
    "\n",
    "So we obtain:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\hat{\\beta}} = \\boldsymbol{\\beta} + (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\epsilon}\n",
    "$$\n",
    "\n",
    "Then, we take the expectation of $\\boldsymbol{\\hat{\\beta}}$:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}(\\boldsymbol{\\hat{\\beta}}) = \\mathbb{E}\\left( \\boldsymbol{\\beta} + (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\epsilon} \\right)\n",
    "$$\n",
    "\n",
    "Since $\\boldsymbol{\\beta}$ is constant, its expectation is simply $\\boldsymbol{\\beta}$:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}(\\boldsymbol{\\hat{\\beta}}) = \\boldsymbol{\\beta} + \\mathbb{E}\\left( (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\epsilon} \\right)\n",
    "$$\n",
    "\n",
    "We also know that $\\mathbb{E}(\\boldsymbol{\\epsilon}) = 0$, so:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}\\left( (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\epsilon} \\right) = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbb{E}(\\boldsymbol{\\epsilon}) = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\cdot 0 = 0\n",
    "$$\n",
    "\n",
    "Therefore, the expectation of $\\boldsymbol{\\hat{\\beta}}$ is:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}(\\boldsymbol{\\hat{\\beta}}) = \\boldsymbol{\\beta} + 0 = \\boldsymbol{\\beta}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to show that $\\text{Var}(\\hat{\\boldsymbol{\\beta}}) = \\sigma^2 (\\mathbf{X}^T \\mathbf{X})^{-1}$, we start again with the expression for the OLS estimator:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\hat{\\beta}} = \\boldsymbol{\\beta} + (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\epsilon}\n",
    "$$\n",
    "\n",
    "The variance of $\\boldsymbol{\\hat{\\beta}}$ is determined by the second term, given that $\\boldsymbol{\\beta}$ is a constant:\n",
    "\n",
    "$$\n",
    "\\text{Var}(\\boldsymbol{\\hat{\\beta}}) = \\text{Var}\\left( (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\epsilon} \\right)\n",
    "$$\n",
    "\n",
    "We use the matrix variance formula:\n",
    "\n",
    "$$\n",
    "\\text{Var}(\\mathbf{A} \\boldsymbol{\\epsilon}) = \\mathbf{A} \\text{Var}(\\boldsymbol{\\epsilon}) \\mathbf{A}^T\n",
    "$$\n",
    "\n",
    "where $\\mathbf{A}$ is the matrix $(\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T$ and $\\boldsymbol{\\epsilon}$ is the random error vector. So, we get:\n",
    "\n",
    "$$\n",
    "\\text{Var}\\left( (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\epsilon} \\right) = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\text{Var}(\\boldsymbol{\\epsilon}) \\mathbf{X} (\\mathbf{X}^T \\mathbf{X})^{-1}\n",
    "$$\n",
    "\n",
    "Since $\\text{Var}(\\boldsymbol{\\epsilon}) = \\sigma^2 \\mathbf{I}$, the above equation simplifies to:\n",
    "\n",
    "$$\n",
    "\\text{Var}\\left( (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\epsilon} \\right) = \\sigma^2 (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{X} (\\mathbf{X}^T \\mathbf{X})^{-1}\n",
    "$$\n",
    "\n",
    "Now, $\\mathbf{X}^T \\mathbf{X} (\\mathbf{X}^T \\mathbf{X})^{-1} = \\mathbf{I}$, so the expression further simplifies to:\n",
    "\n",
    "$$\n",
    "\\text{Var}(\\boldsymbol{\\hat{\\beta}}) = \\sigma^2 (\\mathbf{X}^T \\mathbf{X})^{-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Expectation values for Ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show that\n",
    "$$\n",
    "\\mathbb{E} \\left[ \\hat{\\boldsymbol{\\beta}}^{\\text{Ridge}} \\right] = \\left( \\mathbf{X}^T \\mathbf{X} + \\lambda \\mathbf{I}_{pp} \\right)^{-1} \\mathbf{X}^T \\mathbf{X} \\boldsymbol{\\beta}.\n",
    "$$\n",
    "\n",
    "We see clearly that $ \\mathbb{E} \\left[ \\hat{\\boldsymbol{\\beta}}^{\\text{Ridge}} \\right] \\neq \\mathbb{E} \\left[ \\hat{\\boldsymbol{\\beta}}^{\\text{OLS}} \\right] $ for any $ \\lambda > 0 $.\n",
    "\n",
    "Show also that the variance is\n",
    "\n",
    "$$\n",
    "\\text{Var}\\left( \\hat{\\boldsymbol{\\beta}}^{\\text{Ridge}} \\right) = \\sigma^2 \\left( \\mathbf{X}^T \\mathbf{X} + \\lambda \\mathbf{I} \\right)^{-1} \\mathbf{X}^T \\mathbf{X} \\left( \\left( \\mathbf{X}^T \\mathbf{X} + \\lambda \\mathbf{I} \\right)^{-1} \\right)^T,\n",
    "$$\n",
    "\n",
    "and it is easy to see that if the parameter $ \\lambda $ goes to infinity then the variance of the Ridge parameters $ \\boldsymbol{\\beta} $ goes to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by expressing the Ridge regression estimate:\n",
    "\n",
    "$$\n",
    "\\hat{\\boldsymbol{\\beta}}^{\\text{Ridge}} = \\left( \\mathbf{X}^T \\mathbf{X} + \\lambda \\mathbf{I}_{pp} \\right)^{-1} \\mathbf{X}^T \\mathbf{y}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\mathbf{X}$ is the design matrix with shape $(n \\times p)$.\n",
    "- $\\mathbf{y}$ is the response vector with shape $(n \\times 1)$.\n",
    "- $\\lambda$ is the regularization parameter.\n",
    "- $\\mathbf{I}_{pp}$ is the $p \\times p$ identity matrix.\n",
    "\n",
    "Again, the linear model we are using is expressed by:\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\n",
    "$$\n",
    "\n",
    "where $\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(0, \\sigma^2)$.\n",
    "\n",
    "By substituing the above expression in the $\\hat{\\boldsymbol{\\beta}}^{\\text{Ridge}}$ one, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{\\boldsymbol{\\beta}}^{\\text{Ridge}} \n",
    "& = \\left( \\mathbf{X}^T \\mathbf{X} + \\lambda \\mathbf{I} \\right)^{-1} \\mathbf{X}^T (\\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}) \\\\\n",
    "& = \\left( \\mathbf{X}^T \\mathbf{X} + \\lambda \\mathbf{I} \\right)^{-1} \\mathbf{X}^T \\mathbf{X} \\boldsymbol{\\beta} + \\left( \\mathbf{X}^T \\mathbf{X} + \\lambda \\mathbf{I} \\right)^{-1} \\mathbf{X}^T \\boldsymbol{\\epsilon}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "We then take the expectation of both sides:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}\\left[\\hat{\\boldsymbol{\\beta}}^{\\text{Ridge}}\\right] = \\mathbb{E} \\left[ \\left( \\mathbf{X}^T \\mathbf{X} + \\lambda \\mathbf{I} \\right)^{-1} \\mathbf{X}^T \\mathbf{X} \\boldsymbol{\\beta} \\right] + \\mathbb{E}\\left[ \\left( \\mathbf{X}^T \\mathbf{X} + \\lambda \\mathbf{I} \\right)^{-1} \\mathbf{X}^T \\boldsymbol{\\epsilon} \\right]\n",
    "$$\n",
    "\n",
    "Since $ \\mathbb{E}[\\boldsymbol{\\epsilon}] = 0 $, the second term vanishes. Thus, the expectation of the Ridge estimator is:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}\\left[\\hat{\\boldsymbol{\\beta}}^{\\text{Ridge}}\\right] = \\left( \\mathbf{X}^T \\mathbf{X} + \\lambda \\mathbf{I} \\right)^{-1} \\mathbf{X}^T \\mathbf{X} \\boldsymbol{\\beta}\n",
    "$$\n",
    "\n",
    "Note that when $ \\lambda = 0 $, we recover the ordinary least squares (OLS) estimator:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}\\left[\\hat{\\boldsymbol{\\beta}}^{\\text{OLS}}\\right] = \\mathbb{E}\\left[\\left(\\mathbf{X}^T \\mathbf{X}\\right)^{-1} \\mathbf{X}^T \\mathbf{y}\\right] = \\boldsymbol{\\beta}\n",
    "$$\n",
    "\n",
    "However, for any $ \\lambda > 0 $, we have:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}\\left[\\hat{\\boldsymbol{\\beta}}^{\\text{Ridge}}\\right] \\neq \\boldsymbol{\\beta}\n",
    "$$\n",
    "\n",
    "This shows that the Ridge estimator is biased.\n",
    "\n",
    "Now we compute the variance of the Ridge estimator $ \\hat{\\boldsymbol{\\beta}}^{\\text{Ridge}} $:\n",
    "\n",
    "$$\n",
    "\\text{Var}\\left( \\hat{\\boldsymbol{\\beta}}^{\\text{Ridge}} \\right) = \\text{Var}\\left( \\left( \\mathbf{X}^T \\mathbf{X} + \\lambda \\mathbf{I} \\right)^{-1} \\mathbf{X}^T \\boldsymbol{\\epsilon} \\right)\n",
    "$$\n",
    "\n",
    "Using the fact that $ \\text{Var}(\\mathbf{A} \\boldsymbol{\\epsilon}) = \\mathbf{A} \\text{Var}(\\boldsymbol{\\epsilon}) \\mathbf{A}^T $, we get:\n",
    "\n",
    "$$\n",
    "\\text{Var}\\left( \\hat{\\boldsymbol{\\beta}}^{\\text{Ridge}} \\right) = \\left( \\mathbf{X}^T \\mathbf{X} + \\lambda \\mathbf{I} \\right)^{-1} \\mathbf{X}^T \\text{Var}(\\boldsymbol{\\epsilon}) \\mathbf{X} \\left( \\left( \\mathbf{X}^T \\mathbf{X} + \\lambda \\mathbf{I} \\right)^{-1} \\right)^T\n",
    "$$\n",
    "\n",
    "Substitute $ \\text{Var}(\\boldsymbol{\\epsilon}) = \\sigma^2 \\mathbf{I} $:\n",
    "\n",
    "$$\n",
    "\\text{Var}\\left( \\hat{\\boldsymbol{\\beta}}^{\\text{Ridge}} \\right) = \\sigma^2 \\left( \\mathbf{X}^T \\mathbf{X} + \\lambda \\mathbf{I} \\right)^{-1} \\mathbf{X}^T \\mathbf{X} \\left( \\left( \\mathbf{X}^T \\mathbf{X} + \\lambda \\mathbf{I} \\right)^{-1} \\right)^T\n",
    "$$\n",
    "\n",
    "Thus, the variance of the Ridge estimator is:\n",
    "\n",
    "$$\n",
    "\\text{Var}\\left( \\hat{\\boldsymbol{\\beta}}^{\\text{Ridge}} \\right) = \\sigma^2 \\left( \\mathbf{X}^T \\mathbf{X} + \\lambda \\mathbf{I} \\right)^{-1} \\mathbf{X}^T \\mathbf{X} \\left( \\left( \\mathbf{X}^T \\mathbf{X} + \\lambda \\mathbf{I} \\right)^{-1} \\right)^T\n",
    "$$\n",
    "\n",
    "We can see that:\n",
    "\n",
    "- As $ \\lambda \\to 0 $, the variance of $ \\hat{\\boldsymbol{\\beta}}^{\\text{Ridge}} $ approaches the variance of the OLS estimator.\n",
    "- As $ \\lambda \\to \\infty $, the variance decreases to zero because the regularization term dominates and shrinks the coefficients toward zero, making the estimate less sensitive to noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Citations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to solve the above two exercises, for which the complete text can be found at https://compphysics.github.io/MachineLearning/doc/LectureNotes/_build/html/week36.html#expectation-value-and-variance-for-boldsymbol-beta, we mainly used the lectures notes related to week 36 of the course *'Applied Data Analysis and Machine Learning'*, published by professor Morten Hjorth-Jensen, and available at https://compphysics.github.io/MachineLearning/doc/LectureNotes/_build/html/week36.html#expectation-value-and-variance-for-boldsymbol-beta.\n",
    "\n",
    "Additionally, we took advantage of ChatGPT 4o for some computations and to obtain some parts of the LaTeX code."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
