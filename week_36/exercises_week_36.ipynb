{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Pedro Alonso Lopez Torres**\n",
    "- **Elisa Ottoboni**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Analytical exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Expression for Ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show that the optimal parameters are equal to\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_{\\text{Ridge}} = \\left( X^TX + \\lambda I \\right)^{-1} X^T y\n",
    "$$\n",
    "\n",
    "with $I$ being a $p \\times p$ identity matrix.\n",
    "\n",
    "We start by expressing the optimization formula of $\\operatorname{MSE}$, our cost function $C(\\beta)$, with the addition of a regularization parameter $\\lambda$\n",
    "\n",
    "$$\n",
    "\\min_{\\beta \\in \\mathbb{R}^p} \\frac{1}{n} \\| y - X\\beta \\|_2^2 + \\lambda \\| \\beta \\|_2^2\n",
    "$$\n",
    "\n",
    "where we have used the definition of a norm-2 vector, that is\n",
    "\n",
    "$$\n",
    "\\| \\mathbf{x} \\|_2 = \\sqrt{ \\sum_i x_i^2 }\n",
    "$$\n",
    "\n",
    "In order to obtain the $\\hat{\\beta}_{\\text{Ridge}}$ parameters, we start by expanding the above $\\operatorname{MSE}$ equation\n",
    "\n",
    "\\begin{align*}\n",
    "C(\\boldsymbol{\\beta}) \n",
    "& = \\frac{1}{n} (\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta})^T(\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta}) + \\lambda \\boldsymbol{\\beta}^T \\boldsymbol{\\beta} \\\\\n",
    "& = \\frac{1}{n} (\\mathbf{y}^T \\mathbf{y} - \\mathbf{y}^T \\mathbf{X} \\boldsymbol{\\beta} - \\boldsymbol{\\beta}^T \\mathbf{X}^T \\mathbf{y} + \\boldsymbol{\\beta}^T \\mathbf{X}^T \\mathbf{X} \\boldsymbol{\\beta}) + \\lambda \\boldsymbol{\\beta}^T \\boldsymbol{\\beta} \\\\\n",
    "& = \\frac{1}{n} \\left( \\mathbf{y}^T \\mathbf{y} - 2 \\boldsymbol{\\beta}^T \\mathbf{X}^T \\mathbf{y} + \\boldsymbol{\\beta}^T \\mathbf{X}^T \\mathbf{X} \\boldsymbol{\\beta} \\right) + \\lambda \\boldsymbol{\\beta}^T \\boldsymbol{\\beta}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "Now, we take the derivative of $C(\\boldsymbol{\\beta})$ with respect to $\\boldsymbol{\\beta}$ and set it equal to 0 to minimize it\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C(\\boldsymbol{\\beta})}{\\partial \\boldsymbol{\\beta}} = - \\frac{2}{n} \\mathbf{X}^T \\mathbf{y} + \\frac{2}{n} \\mathbf{X}^T \\mathbf{X} \\boldsymbol{\\beta} + 2 \\lambda \\boldsymbol{\\beta} = 0\n",
    "$$\n",
    "\n",
    "We divide the equation by 2 to simplify it:\n",
    "\n",
    "$$\n",
    "-\\frac{1}{n} \\mathbf{X}^T \\mathbf{y} + \\frac{1}{n} \\mathbf{X}^T \\mathbf{X} \\boldsymbol{\\beta} + \\lambda \\boldsymbol{\\beta} = 0\n",
    "$$\n",
    "\n",
    "We then rearrange the terms to isolate $\\beta$ and multiply both sides by $n$:\n",
    "\n",
    "$$\n",
    "\\frac{1}{n} \\mathbf{X}^T \\mathbf{X} \\boldsymbol{\\beta} + \\lambda \\boldsymbol{\\beta} = \\frac{1}{n} \\mathbf{X}^T \\mathbf{y}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{X}^T \\mathbf{X} \\boldsymbol{\\beta} + n\\lambda \\boldsymbol{\\beta} = \\mathbf{X}^T \\mathbf{y}\n",
    "$$\n",
    "\n",
    "Since $\\lambda$ is a number, we rewrite $n\\lambda$ as just $\\lambda$, knowing that this could be considered an abuse of notation.\n",
    "\n",
    "$$\n",
    "\\left( \\mathbf{X}^T \\mathbf{X} + \\lambda \\mathbf{I} \\right) \\boldsymbol{\\beta} = \\mathbf{X}^T \\mathbf{y}\n",
    "$$\n",
    "\n",
    "Finally, we obtain the expression for the optimal Ridge regression parameters.\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\beta}_{\\text{Ridge}} = \\left( \\mathbf{X}^T \\mathbf{X} + \\lambda \\mathbf{I} \\right)^{-1} \\mathbf{X}^T \\mathbf{y}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we also derive the $\\boldsymbol{\\beta}_{\\text{OLS}}$ parameters. We start by expanding the cost function\n",
    "\n",
    "\\begin{align*}\n",
    "C(\\boldsymbol{\\beta}) \n",
    "& = \\frac{1}{n} \\left( \\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta} \\right)^T \\left( \\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta} \\right) \\\\\n",
    "& = \\frac{1}{n} \\left( \\mathbf{y}^T\\mathbf{y} - \\mathbf{y}^T\\mathbf{X}\\boldsymbol{\\beta} - \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{y} + \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta} \\right) \\\\\n",
    "& = \\frac{1}{n} \\left( \\mathbf{y}^T\\mathbf{y} - 2\\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{y} + \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta} \\right)\n",
    "\\end{align*}\n",
    "\n",
    "We can now differentiate it and set it to 0\n",
    "\n",
    "$$\n",
    "\\frac{dC(\\boldsymbol{\\beta})}{d\\boldsymbol{\\beta}} = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "-2\\mathbf{y}^T\\mathbf{X} + 2\\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{X} = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "2\\left( -\\mathbf{y}^T\\mathbf{X} + \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{X} \\right) = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\beta}^T = \\mathbf{y}^T\\mathbf{X} \\left( \\mathbf{X}^T\\mathbf{X} \\right)^{-1}\n",
    "$$\n",
    "\n",
    "At the end, we obtain the parameters we were looking for\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\beta}_{\\text{OLS}} = \\left( \\mathbf{X}^T\\mathbf{X} \\right)^{-1}\\mathbf{X}^T\\mathbf{y}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) The singular value decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
