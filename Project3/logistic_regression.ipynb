{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files and directories in 'C:\\Users\\lotop\\.cache\\kagglehub\\datasets\\fmena14\\volcanoesvenus\\versions\\1':\n",
      "['Volcanoes_test', 'Volcanoes_train']\n",
      "Files in 'Volcanoes_train':\n",
      "['train_images.csv', 'train_labels.csv']\n",
      "Files in 'Volcanoes_test':\n",
      "['test_images.csv', 'test_labels.csv']\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"fmena14/volcanoesvenus\")\n",
    "\n",
    "# Get the list of all files and directories \n",
    "files = os.listdir(path)\n",
    "print(f\"Files and directories in '{path}':\\n{files}\")  \n",
    "\n",
    "train_files = os.listdir(os.path.join(path, 'Volcanoes_train'))\n",
    "test_files = os.listdir(os.path.join(path, 'Volcanoes_test'))\n",
    "\n",
    "# Check files inside the 'Volcanoes_test' and 'Volcanoes_train' files\n",
    "print(f\"Files in 'Volcanoes_train':\\n{train_files}\")\n",
    "print(f\"Files in 'Volcanoes_test':\\n{test_files}\")\n",
    "\n",
    "# We use 'header=None' for 'X_train' and 'X_test' so pandas doesn't treat the first row as indeces\n",
    "X_train = pd.read_csv(os.path.join(path, 'Volcanoes_train', 'train_images.csv'), header=None)\n",
    "y_train = pd.read_csv(os.path.join(path, 'Volcanoes_train', 'train_labels.csv'))\n",
    "X_test = pd.read_csv(os.path.join(path, 'Volcanoes_test', 'test_images.csv'), header=None)\n",
    "y_test = pd.read_csv(os.path.join(path, 'Volcanoes_test', 'test_labels.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0      1      2      3      4      5      6      7      8      9      ...  \\\n",
      "0     95    101     99    103     95     86     96     89     70    104  ...   \n",
      "1     91     92     91     89     92     93     96    101    107    104  ...   \n",
      "2     87     70     72     74     84     78     93    104    106    106  ...   \n",
      "3      0      0      0      0      0      0      0      0      0      0  ...   \n",
      "4    114    118    124    119     95    118    105    116    123    112  ...   \n",
      "\n",
      "   12090  12091  12092  12093  12094  12095  12096  12097  12098  12099  \n",
      "0    111    107     92     89    103     99    117    116    118     96  \n",
      "1    103     92     93     95     98    105    104    100     90     81  \n",
      "2     84     71     95    102     94     80     91     80     84     90  \n",
      "3     94     81     89     84     80     90     92     80     88     96  \n",
      "4    116    113    102     93    109    104    106    117    111    115  \n",
      "\n",
      "[5 rows x 12100 columns]\n",
      "--------------------------------------------------------------------------------\n",
      "   Volcano?  Type  Radius  Number Volcanoes\n",
      "0         1   3.0   17.46               1.0\n",
      "1         0   NaN     NaN               NaN\n",
      "2         0   NaN     NaN               NaN\n",
      "3         0   NaN     NaN               NaN\n",
      "4         0   NaN     NaN               NaN\n",
      "--------------------------------------------------------------------------------\n",
      "X_train shape: (7000, 12100)\n",
      "y_train shape: (7000, 4)\n",
      "X_test shape: (2734, 12100)\n",
      "y_test shape: (2734, 4)\n",
      "--------------------------------------------------------------------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7000 entries, 0 to 6999\n",
      "Columns: 12100 entries, 0 to 12099\n",
      "dtypes: int64(12100)\n",
      "memory usage: 646.2 MB\n",
      "None\n",
      "--------------------------------------------------------------------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7000 entries, 0 to 6999\n",
      "Data columns (total 4 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   Volcano?          7000 non-null   int64  \n",
      " 1   Type              1000 non-null   float64\n",
      " 2   Radius            1000 non-null   float64\n",
      " 3   Number Volcanoes  1000 non-null   float64\n",
      "dtypes: float64(3), int64(1)\n",
      "memory usage: 218.9 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(X_train.head())\n",
    "print('-'*80)\n",
    "print(y_train.head())\n",
    "print('-'*80)\n",
    "print(f'X_train shape: {X_train.shape}\\ny_train shape: {y_train.shape}\\nX_test shape: {X_test.shape}\\ny_test shape: {y_test.shape}')\n",
    "print('-'*80)\n",
    "print(X_train.info())\n",
    "print('-'*80)\n",
    "print(y_train.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this analysis, we are only interested in identifying the presence or absence of volcanoes. Therefore, the `Volcano?` column will be used as the target variable. Fortunately, it doesn't contain any NaN values. The NaN values in the other columns ('Type', 'Radius', 'Number Volcanoes') are irrelevant for this task and can be safely ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the 'Volcano?' column as the target variable\n",
    "y_train = y_train['Volcano?']\n",
    "y_test = y_test['Volcano?']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main classifiers we want to implement in this analysis are logistic regression and random forest. For the former, we need to normalize our data. For a more consistent approach, we will use data normalization for both the logistic regression and random forest models, even though the latter doesn't really require it.\n",
    "\n",
    "In this particular case, dividing all input values by the highest value ($=255$) is essentially equivalent to using the MinMaxScaler. However, the former approach has the main advantage of computational efficiency, so it's preferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_image_normalizer(X):\n",
    "    '''\n",
    "    Takes as input flattened raw images and normalizes them by dividing its values for the highest value.\n",
    "\n",
    "    Input parameters:\n",
    "    - X: A feature matrix X.\n",
    "\n",
    "    Output:\n",
    "    - X_normalized: The respective normalized feature matrix X.\n",
    "    '''\n",
    "\n",
    "    max_value = X.values.max()\n",
    "    X_normalized = X / max_value\n",
    "\n",
    "    return X_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization of both train and test inputs\n",
    "\n",
    "X_train = raw_image_normalizer(X_train)\n",
    "X_test = raw_image_normalizer(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After normalizing the data, we can proceed to the dimensionality reduction part. \n",
    "\n",
    "### Why use dimensionality reduction?\n",
    "\n",
    "In the training data, we have a total number of features ($p$) equal to $12 \\, 100$, and a total number of samples ($n$) equal to $5 \\, 000$. This property of our feature matrix ($p >> n$) can cause several problems, such as \n",
    "\n",
    "- **Overfitting**: Having many more features than samples can lead to noise fitting.\n",
    "- Slow computation: Dimensionality reduction methods such as PCA on large matrices can be computationally expensive.\n",
    "- Numerical instability: Dimensionality reduction mitigates numerical instability in subsequent steps (e.g., model fitting).\n",
    "\n",
    "For all of the above reasons, we will use PCA to reduce the dimensionality of the data. We aim to retain about $95/%$ of the total variance.\n",
    "\n",
    "### Why do we focus on the retained variance rather than the number of dimensions?\n",
    "\n",
    "Rather than arbitrarily choosing the number of dimensions to reduce to, it is generally preferable to choose the number of dimensions that add up to a sufficiently large portion of the variance (e.g., $95\\%$). Unless, of course, we are reducing the dimensionality for data visualization - in which case we will generally want to reduce the dimensionality to $2$ or $3$.\n",
    "\n",
    "### References\n",
    "\n",
    "- ML Jupyter book: https://compphysics.github.io/MachineLearning/doc/LectureNotes/_build/html/chapter8.html\n",
    "- SL book (for the part regarding $p >> n$ and numerical instability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Fit PCA on the training data (calculate principal components)\n",
    "pca = PCA(n_components=0.95)\n",
    "X_train_reduced = pca.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data using the same PCA model\n",
    "X_test_reduced = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components retained: 1537\n",
      "Cumulative explained variance: [0.68638636 0.71755797 0.72901591 ... 0.94992152 0.94996224 0.9500029 ]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of components retained: {pca.n_components_}\")\n",
    "print(f\"Cumulative explained variance: {pca.explained_variance_ratio_.cumsum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dimensionality was successfully reduced from $12 \\, 100$ to $1 \\, 537$, while retaining $95\\%$ of the variance. We managed to achieve our objective in terms of dimensionality reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversampling\n",
    "\n",
    "Finally, we need to address the fact that the number of volcanoes' images in the training data is just a small portion of the total number of images. For this reason, we will implement **SMOTE** (Synthetic Minority Over-sampling Technique). This algorithm allows us to generates synthetic samples by interpolating between existing minority class samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train_reduced, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 6000, 0: 6000})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# We verify the new class distribution\n",
    "print(Counter(y_train_balanced))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class LogisticRegressionSGD:\n",
    "    \"\"\"\n",
    "    Logistic Regression classifier using Stochastic Gradient Descent (SGD) optimization.\n",
    "\n",
    "    This class supports logistic regression with various features such as:\n",
    "    - Mini-batch SGD with momentum.\n",
    "    - L2 regularization.\n",
    "    - Early stopping with patience.\n",
    "    - Hyperparameter analysis for tuning learning rate and regularization parameters.\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate=0.01, num_iterations=1000, batch_size=32, momentum=0.0, l2_lambda=0.0, patience=10, tol=1e-4, verbose=False):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iterations = num_iterations\n",
    "        self.batch_size = batch_size\n",
    "        self.momentum = momentum\n",
    "        self.l2_lambda = l2_lambda\n",
    "        self.patience = patience  # Early stopping patience\n",
    "        self.tol = tol  # Minimum improvement threshold for early stopping\n",
    "        self.beta = None  # Vector of weights for the logistic regression model\n",
    "        self.verbose = verbose\n",
    "        self.loss_history = []\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def compute_loss(self, X, y):\n",
    "        y_pred = self.sigmoid(X @ self.beta)\n",
    "        # Clip y_pred to avoid log(0) issues\n",
    "        y_pred = np.clip(y_pred, 1e-10, 1 - 1e-10)\n",
    "        loss = -np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
    "        if self.l2_lambda > 0:\n",
    "            loss += (self.l2_lambda / 2) * np.sum(self.beta ** 2)\n",
    "        return loss\n",
    "\n",
    "    def SGDfit(self, X, y):\n",
    "        samples, features = X.shape\n",
    "        self.beta = np.random.randn(features, 1) * 0.01  # Small initialization\n",
    "        velocity = np.zeros((features, 1))\n",
    "\n",
    "        # Initialize best_loss to infinity to ensure any first epoch loss will be smaller, allowing it to update.\n",
    "        best_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "\n",
    "        for epoch in range(self.num_iterations):\n",
    "            # Shuffle data\n",
    "            shuffled_indices = np.random.permutation(samples)\n",
    "            X_shuffled = X[shuffled_indices]\n",
    "            y_shuffled = y[shuffled_indices].reshape(-1, 1)\n",
    "\n",
    "            # Loop over mini-batches\n",
    "            for i in range(0, samples, self.batch_size):\n",
    "                X_batch = X_shuffled[i:i + self.batch_size]\n",
    "                y_batch = y_shuffled[i:i + self.batch_size]\n",
    "\n",
    "                # Compute predictions\n",
    "                y_predicted = self.sigmoid(X_batch @ self.beta)\n",
    "\n",
    "                # Compute the gradient with L2 regularization\n",
    "                gradient = (X_batch.T @ (y_predicted - y_batch)) / self.batch_size\n",
    "                if self.l2_lambda > 0:\n",
    "                    gradient += self.l2_lambda * self.beta\n",
    "\n",
    "                # Update velocity and beta with momentum\n",
    "                velocity = self.momentum * velocity + self.learning_rate * gradient\n",
    "                self.beta -= velocity\n",
    "\n",
    "            # Calculate and check loss for early stopping\n",
    "            current_loss = self.compute_loss(X, y)\n",
    "            self.loss_history.append(current_loss)  # Track loss over epochs\n",
    "\n",
    "            # Print loss every 100 epochs if verbose is True\n",
    "            if self.verbose and epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}: Training Loss = {current_loss:.4f}\")\n",
    "\n",
    "            if current_loss < best_loss - self.tol:\n",
    "                best_loss = current_loss\n",
    "                patience_counter = 0  # Reset counter if improvement\n",
    "            else:\n",
    "                patience_counter += 1  # Increment counter if no significant improvement\n",
    "\n",
    "            if patience_counter >= self.patience:\n",
    "                print(f\"Stopping early at epoch {epoch + 1}\")\n",
    "                break\n",
    "\n",
    "    def predict(self, X):\n",
    "        linear_model = X @ self.beta\n",
    "        y_predicted = self.sigmoid(linear_model)\n",
    "        return [1 if i >= 0.5 else 0 for i in y_predicted]\n",
    "\n",
    "    def plot_loss_history(self):\n",
    "        plt.plot(self.loss_history)\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Training Loss')\n",
    "        plt.title('Training Loss over Epochs')\n",
    "        plt.show()\n",
    "\n",
    "    def analyze_hyperparameters(\n",
    "        self,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        learning_rates=None,\n",
    "        regularization_terms=None,\n",
    "        num_iterations=5000,\n",
    "        save_to_pdf=True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Perform grid search to optimize learning rate and regularization parameters,\n",
    "        including metrics like precision, recall, and F1-score.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        tuple : (best_learning_rate, best_lambda, best_f1, best_model)\n",
    "            - best_learning_rate (float): Optimal learning rate based on highest F1-score.\n",
    "            - best_lambda (float): Optimal regularization term (λ) based on highest F1-score.\n",
    "            - best_f1 (float): Highest F1-score achieved.\n",
    "            - best_model (LogisticRegressionSGD): Model instance trained with best parameters.\n",
    "        \"\"\"\n",
    "        if learning_rates is None:\n",
    "            learning_rates = np.logspace(-5, 1, 70)\n",
    "        if regularization_terms is None:\n",
    "            regularization_terms = np.logspace(-4, 1, 50)\n",
    "\n",
    "        # Best parameters initialization\n",
    "        best_f1 = 0\n",
    "        best_learning_rate = None\n",
    "        best_lambda = 0.0\n",
    "        best_model = None\n",
    "\n",
    "        # Analyze learning rates\n",
    "        print(\"Analyzing learning rates...\")\n",
    "        for lr in learning_rates:\n",
    "            model = LogisticRegressionSGD(\n",
    "                learning_rate=lr,\n",
    "                num_iterations=num_iterations,\n",
    "                batch_size=self.batch_size,\n",
    "                momentum=self.momentum,\n",
    "                l2_lambda=0.01,  # Temporary regularization for LR search\n",
    "                patience=self.patience,\n",
    "                tol=self.tol\n",
    "            )\n",
    "            model.SGDfit(X_train, y_train)\n",
    "            \n",
    "            # Predictions and metrics\n",
    "            predictions = model.predict(X_test)\n",
    "            f1 = f1_score(y_test, predictions)\n",
    "            precision = precision_score(y_test, predictions)\n",
    "            recall = recall_score(y_test, predictions)\n",
    "            accuracy = accuracy_score(y_test, predictions)\n",
    "            \n",
    "            # Print verbose results\n",
    "            if self.verbose:\n",
    "                print(f\"Learning rate: {lr:.6f}, \"\n",
    "                      f\"F1-score: {f1:.4f}, \"\n",
    "                      f\"Precision: {precision:.4f}, \"\n",
    "                      f\"Recall: {recall:.4f}, \"\n",
    "                      f\"Accuracy: {accuracy:.4f}\")\n",
    "            \n",
    "            # Track best parameters based on F1-score\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_learning_rate = lr\n",
    "                best_model = model\n",
    "\n",
    "        print(f\"\\nBest learning rate found: {best_learning_rate:.6f} (F1-score: {best_f1:.4f})\")\n",
    "        print('-' * 60)\n",
    "\n",
    "        # Analyze regularization terms\n",
    "        print(\"\\nAnalyzing regularization terms...\")\n",
    "        for reg in regularization_terms:\n",
    "            model = LogisticRegressionSGD(\n",
    "                learning_rate=best_learning_rate,\n",
    "                num_iterations=num_iterations,\n",
    "                batch_size=self.batch_size,\n",
    "                momentum=self.momentum,\n",
    "                l2_lambda=reg,\n",
    "                patience=self.patience,\n",
    "                tol=self.tol\n",
    "            )\n",
    "            model.SGDfit(X_train, y_train)\n",
    "            \n",
    "            # Predictions and metrics\n",
    "            predictions = model.predict(X_test)\n",
    "            f1 = f1_score(y_test, predictions)\n",
    "            precision = precision_score(y_test, predictions)\n",
    "            recall = recall_score(y_test, predictions)\n",
    "            accuracy = accuracy_score(y_test, predictions)\n",
    "            \n",
    "            # Print verbose results\n",
    "            if self.verbose:\n",
    "                print(f\"Lambda: {reg:.6f}, \"\n",
    "                      f\"F1-score: {f1:.4f}, \"\n",
    "                      f\"Precision: {precision:.4f}, \"\n",
    "                      f\"Recall: {recall:.4f}, \"\n",
    "                      f\"Accuracy: {accuracy:.4f}\")\n",
    "            \n",
    "            # Track best parameters based on F1-score\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_lambda = reg\n",
    "                best_model = model\n",
    "\n",
    "        print(f\"\\nBest regularization term found: {best_lambda:.6f} (F1-score: {best_f1:.4f})\")\n",
    "        return best_learning_rate, best_lambda, best_f1, best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression data implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before fine-tuning of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegressionSGD(\n",
    "    learning_rate=0.01,\n",
    "    num_iterations=1000,\n",
    "    batch_size=32,\n",
    "    momentum=0.9,\n",
    "    l2_lambda=0.1,\n",
    "    patience=10,\n",
    "    tol=1e-4,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Training Loss = 0.8328\n",
      "Stopping early at epoch 17\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model.SGDfit(X_train_balanced, y_train_balanced.values)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = model.predict(X_test_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report on Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  No Volcano       0.98      0.64      0.77      2300\n",
      "     Volcano       0.33      0.93      0.48       434\n",
      "\n",
      "    accuracy                           0.68      2734\n",
      "   macro avg       0.65      0.78      0.63      2734\n",
      "weighted avg       0.88      0.68      0.72      2734\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Evaluation of the model\n",
    "print(\"Classification Report on Test Set:\")\n",
    "print(classification_report(y_test, predictions, target_names=[\"No Volcano\", \"Volcano\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After fine-tuning of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing learning rates...\n",
      "Stopping early at epoch 11\n",
      "Learning rate: 0.000100, F1-score: 0.4738, Precision: 0.3206, Recall: 0.9078, Accuracy: 0.6800\n",
      "Stopping early at epoch 11\n",
      "Learning rate: 0.000278, F1-score: 0.4926, Precision: 0.3358, Recall: 0.9240, Accuracy: 0.6979\n",
      "Stopping early at epoch 11\n",
      "Learning rate: 0.000774, F1-score: 0.5240, Precision: 0.3629, Recall: 0.9424, Accuracy: 0.7282\n",
      "Stopping early at epoch 11\n",
      "Learning rate: 0.002154, F1-score: 0.5321, Precision: 0.3704, Recall: 0.9447, Accuracy: 0.7363\n",
      "Stopping early at epoch 11\n",
      "Learning rate: 0.005995, F1-score: 0.4982, Precision: 0.3392, Recall: 0.9378, Accuracy: 0.7001\n",
      "Stopping early at epoch 11\n",
      "Learning rate: 0.016681, F1-score: 0.5245, Precision: 0.3645, Recall: 0.9355, Accuracy: 0.7308\n",
      "Stopping early at epoch 17\n",
      "Learning rate: 0.046416, F1-score: 0.4964, Precision: 0.3355, Recall: 0.9539, Accuracy: 0.6928\n",
      "Stopping early at epoch 27\n",
      "Learning rate: 0.129155, F1-score: 0.4042, Precision: 0.2656, Recall: 0.8456, Accuracy: 0.6042\n",
      "Stopping early at epoch 11\n",
      "Learning rate: 0.359381, F1-score: 0.3300, Precision: 0.2176, Recall: 0.6820, Accuracy: 0.5604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lotop\\AppData\\Local\\Temp\\ipykernel_6312\\1760158216.py:30: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping early at epoch 19\n",
      "Learning rate: 1.000000, F1-score: 0.3280, Precision: 0.2161, Recall: 0.6797, Accuracy: 0.5578\n",
      "\n",
      "Best learning rate found: 0.002154 (F1-score: 0.5321)\n",
      "------------------------------------------------------------\n",
      "\n",
      "Analyzing regularization terms...\n",
      "Stopping early at epoch 11\n",
      "Lambda: 0.001000, F1-score: 0.5384, Precision: 0.3765, Recall: 0.9447, Accuracy: 0.7429\n",
      "Stopping early at epoch 11\n",
      "Lambda: 0.002783, F1-score: 0.5177, Precision: 0.3569, Recall: 0.9424, Accuracy: 0.7213\n",
      "Stopping early at epoch 11\n",
      "Lambda: 0.007743, F1-score: 0.5328, Precision: 0.3710, Recall: 0.9447, Accuracy: 0.7370\n",
      "Stopping early at epoch 11\n",
      "Lambda: 0.021544, F1-score: 0.5128, Precision: 0.3516, Recall: 0.9470, Accuracy: 0.7143\n",
      "Stopping early at epoch 11\n",
      "Lambda: 0.059948, F1-score: 0.4869, Precision: 0.3283, Recall: 0.9424, Accuracy: 0.6847\n",
      "Stopping early at epoch 11\n",
      "Lambda: 0.166810, F1-score: 0.4736, Precision: 0.3178, Recall: 0.9286, Accuracy: 0.6723\n",
      "Stopping early at epoch 19\n",
      "Lambda: 0.464159, F1-score: 0.3899, Precision: 0.2562, Recall: 0.8157, Accuracy: 0.5947\n",
      "Stopping early at epoch 13\n",
      "Lambda: 1.291550, F1-score: 0.3891, Precision: 0.2546, Recall: 0.8249, Accuracy: 0.5889\n",
      "Stopping early at epoch 13\n",
      "Lambda: 3.593814, F1-score: 0.3105, Precision: 0.2031, Recall: 0.6590, Accuracy: 0.5355\n",
      "Stopping early at epoch 25\n",
      "Lambda: 10.000000, F1-score: 0.2644, Precision: 0.1742, Recall: 0.5484, Accuracy: 0.5157\n",
      "\n",
      "Best regularization term found: 0.001000 (F1-score: 0.5384)\n",
      "Best Learning Rate: 0.002154434690031882\n",
      "Best Regularization Term (Lambda): 0.001\n",
      "Best F1-Score: 0.5384110308601444\n"
     ]
    }
   ],
   "source": [
    "# Define ranges for hyperparameter search\n",
    "learning_rates = np.logspace(-4, 0, 10)  # Learning rates between 1e-4 and 1\n",
    "regularization_terms = np.logspace(-3, 1, 10)  # Regularization terms between 1e-3 and 10\n",
    "\n",
    "# Perform hyperparameter analysis\n",
    "best_lr, best_lambda, best_f1, best_model = model.analyze_hyperparameters(\n",
    "    X_train_balanced,\n",
    "    y_train_balanced.values,\n",
    "    X_test_reduced,\n",
    "    y_test.values,\n",
    "    learning_rates=learning_rates,\n",
    "    regularization_terms=regularization_terms,\n",
    "    num_iterations=1000  # Adjust iterations if needed\n",
    ")\n",
    "\n",
    "# Display the best hyperparameters and F1-score\n",
    "print(f\"Best Learning Rate: {best_lr}\")\n",
    "print(f\"Best Regularization Term (Lambda): {best_lambda}\")\n",
    "print(f\"Best F1-Score: {best_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model with optimal hyperparameters\n",
    "final_model = LogisticRegressionSGD(\n",
    "    learning_rate=best_lr,           # Best learning rate found\n",
    "    num_iterations=1000,             # Number of iterations for training\n",
    "    batch_size=32,                   # Batch size for SGD\n",
    "    momentum=0.9,                    # Momentum to stabilize training\n",
    "    l2_lambda=best_lambda,           # Best regularization term found\n",
    "    patience=10,                     # Early stopping patience\n",
    "    tol=1e-4,                        # Minimum improvement threshold\n",
    "    verbose=True                     # Enable detailed output\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Training Loss = 0.7659\n",
      "Stopping early at epoch 11\n"
     ]
    }
   ],
   "source": [
    "# Train the final model\n",
    "final_model.SGDfit(X_train_balanced, y_train_balanced.values)\n",
    "\n",
    "# Make predictions on the test set\n",
    "final_predictions = final_model.predict(X_test_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  No Volcano       0.98      0.68      0.81      2300\n",
      "     Volcano       0.36      0.94      0.52       434\n",
      "\n",
      "    accuracy                           0.72      2734\n",
      "   macro avg       0.67      0.81      0.66      2734\n",
      "weighted avg       0.88      0.72      0.76      2734\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Evaluation of the final model\n",
    "print(\"Final Classification Report:\")\n",
    "print(classification_report(y_test, final_predictions, target_names=[\"No Volcano\", \"Volcano\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Confusion matrix visualization\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(y_test, final_predictions)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"No Volcano\", \"Volcano\"], yticklabels=[\"No Volcano\", \"Volcano\"])\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_project3_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
